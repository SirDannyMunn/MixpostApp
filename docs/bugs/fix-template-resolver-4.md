Youâ€™re right.
Letâ€™s stop engineering around the failure and **fix the real problem: the API must always return valid JSON**. Everything else is noise until THAT is reliable.

Here is the **simple, focused correction plan** for your dev â€” no bloat, no over-complication.

---

# âœ… Problem

Claude is returning **malformed JSON**, which breaks the pipeline.
This is the real root cause.

We shouldnâ€™t be patching downstream logic to guess, recover, or rescue content.
We should make Claude *always* return valid structured JSON.

---

# ğŸ¯ Goal

Make `runJsonContentCall()` **boring and deterministic**:

* LLM â†’ ALWAYS valid JSON
* JSON â†’ decode cleanly
* Schema â†’ pass reliably

No heroics.
No â€œsmart recoveryâ€.
No complicated fallback machinery.

---

# ğŸ”§ Fix Strategy

Two changes solve 95% of these failures.

---

## **1ï¸âƒ£ Fix Prompting â€” Make JSON Return Non-Negotiable**

Claude fails when:

* prompt is long
* system prompt is fuzzy
* too many context blocks
* output scaffolding not enforced

### Update the instruction to Claude to something like this:

```
You must respond ONLY with valid JSON.
Do not include explanations, markdown, or prose.

Return exactly:
{
  "content": "<final post text>"
}

If you cannot produce content for any reason,
return:
{
  "content": ""
}
```

Then explicitly forbid text outside JSON:

```
Do NOT include commentary.
Do NOT return text before or after JSON.
Do NOT stream partial JSON.
```

Claude responds cleanly when the contract is blunt and narrow.

---

## **2ï¸âƒ£ Reduce Cognitive Load on LLM**

Claude breaks JSON most often because it is overloaded.

Right now you are giving it:

* template structure
* swipe structures
* giant duplicated business summary
* long user prompt
* constraints
* tone stuff
* references dump

Itâ€™s too much.

Claude is brilliant at reasoning.
Claude is terrible at structured JSON **under stress**.

### Fix:

Remove duplicated junk.
Stop dumping walls of text.
Trim `user_context`.
Shorten instructions.

---

# ğŸ”’ Hard API Settings to Apply

These improve stability massively:

```
response_format: json_object
temperature: 0.4 â€“ 0.6
max_tokens: SAFE HEADROOM (donâ€™t sit at edge)
streaming: OFF
```

Streaming + JSON = chaos.
Claude truncates JSON when token cap hits.
Thatâ€™s what you saw.

---

# ğŸ§ª Testing

After changes, test:

1ï¸âƒ£ Very short prompt
2ï¸âƒ£ Medium prompt
3ï¸âƒ£ Worst-case long prompt

Confirm:

* JSON always closes
* no leaked assistant text
* no markdown
* no emoji outside JSON
* no truncation

---

# ğŸ¯ Acceptance Criteria

This issue is done when:

* Claude never returns invalid JSON in normal use
* You can run 20 generations in a row with zero failures
* You delete 90% of the â€œrecovery logicâ€ we discussed earlier because you donâ€™t need it anymore

---

# ğŸ§  Philosophy

Right now your system was getting complicated because:

* it was cleaning up after the LLM
* instead of making the LLM behave

Thatâ€™s backwards.

We want:
**Strong contract â†’ simple pipeline â†’ predictable behavior**

---

If you want, next I can:

* rewrite your prompt block
* reduce your context payload design
* lock in a stable schema contract
* give you a working Claude JSON template that never breaks

But yes â€” youâ€™re thinking correctly.

Solve the response discipline problem.
Everything else becomes easier.
